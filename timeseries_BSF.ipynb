{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6d18fb-09ac-434d-be24-e041a0c09b43",
   "metadata": {},
   "source": [
    "# Prepare code for change point analysis\n",
    "\n",
    "Metrics:\n",
    "- density overturning stream function\n",
    "- depth overturning stream function\n",
    "- barotropic stream function\n",
    "\n",
    "Process:\n",
    "1. compute metric\n",
    "2. calculate ensemble mean\n",
    "3. subtract mean from each member\n",
    "4. change point analysis\n",
    "\n",
    "# HERE BSF\n",
    "- ## time series\n",
    "- ensemle mean\n",
    "- subtract mean\n",
    "- change point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fb5e162-f52e-41b8-bea1-08c9303a6585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialisation complete\n"
     ]
    }
   ],
   "source": [
    "### INITIALISATION ###\n",
    "\n",
    "import numpy as np          # fundamental package for scientific computing\n",
    "import xarray as xr         # data handling\n",
    "import glob                 # return all file paths that match a specific pattern\n",
    "import pop_tools            # to mask region of interest\n",
    "\n",
    "path = '/Data/gfi/share/ModData/CESM2_LENS2/ocean/monthly/bsf/'\n",
    "files = glob.glob(path + '*.nc')\n",
    "\n",
    "#setting up of regional mask\n",
    "grid_name = 'POP_gx1v7'\n",
    "region_defs = {\n",
    "    'SubpolarAtlantic':[{'match':{'REGION_MASK':[6]}, 'bounds':{'TLAT':[35.0, 80.0],'TLONG':[280.0, 360.0]}}],\n",
    "    'LabradorSea': [{'match': {'REGION_MASK': [8]}, 'bounds': {'TLAT': [35.0, 66.0]}}]}\n",
    "mask3d = pop_tools.region_mask_3d(grid_name, region_defs=region_defs, mask_name='Subpolar Gyre')\n",
    "mask3d = mask3d.sum('region')  \n",
    "\n",
    "# prepare arrays and dictionaries to store files\n",
    "len_time = 3012 # length of time series\n",
    "time_series_min = np.zeros((len_time, len(files)))  # array for min bsf\n",
    "time_series_east = np.zeros((len_time, len(files))) # array for osnap east\n",
    "time_series_west = np.zeros((len_time, len(files))) # array for osnap west\n",
    "\n",
    "# define start and end points of array \n",
    "point_OSNAP_west = (-55, 53)\n",
    "point_OSNAP_center = (-44, 60)\n",
    "point_OSNAP_east = (-7, 56)\n",
    "\n",
    "print('initialisation complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad3e127d-ff01-4fb3-af32-42aab8ddccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_points_on_line(point1,point2,num):\n",
    "    x1, y1 = point1\n",
    "    x2, y2 = point2\n",
    "    m = (y2 - y1)/(x2 - x1)\n",
    "    b = y1 - (m * x1)\n",
    "    x = np.linspace(x1, x2, num=num)\n",
    "    y = m * x + b\n",
    "    return x, y\n",
    "\n",
    "def cross_section(ds, start_point, end_point, number_of_points):\n",
    "\n",
    "    # Compute array of points on the line\n",
    "    x, y = calculate_points_on_line(start_point, end_point, num=number_of_points)\n",
    "\n",
    "    # prepare empty arraâ€š\n",
    "    i_nlats = np.zeros(len(y))\n",
    "    i_nlons = np.zeros(len(y))\n",
    "    \n",
    "    # retrieve numpy arrays from ds U coordinates\n",
    "    ULAT = ds.ULAT.values\n",
    "    ULONG = ds.ULONG.values\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        target_ulat = y[i]\n",
    "        target_ulong = (x[i] + 360) % 360\n",
    "\n",
    "        # Calculate the absolute differences between the target values and ULAT, ULONG\n",
    "        ulat_diff = np.abs(ULAT - target_ulat)\n",
    "        ulong_diff = np.abs(ULONG - target_ulong)\n",
    "\n",
    "        # Calculate the total difference\n",
    "        total_diff = ulat_diff + ulong_diff\n",
    "\n",
    "        # Find the indices of the minimum total difference\n",
    "        min_index = np.unravel_index(np.nanargmin(total_diff), total_diff.shape)\n",
    "        i_nlats[i] = min_index[0]\n",
    "        i_nlons[i] = min_index[1]\n",
    "\n",
    "    # Combine nlat and nlon arrays into a single array of tuples\n",
    "    tuples = np.column_stack((i_nlats, i_nlons))\n",
    "\n",
    "    # Find the unique tuples\n",
    "    unique_tuples = np.unique(tuples, axis=0)\n",
    "\n",
    "    # Separate the unique tuples back into nlat and nlon arrays\n",
    "    unique_nlat = unique_tuples[:, 0]\n",
    "    unique_nlon = unique_tuples[:, 1]\n",
    "    \n",
    "    # convert indices to integers (don't know why they aren't)\n",
    "    nlats = unique_nlat.astype(int)\n",
    "    nlons = unique_nlon.astype(int)\n",
    "    \n",
    "    return nlats, nlons\n",
    "\n",
    "def create_BSF_index(ds, nlats, nlons):\n",
    "\n",
    "    # compute crossection within BSF ds\n",
    "    crossection_ds = ds.isel(nlon=nlons, nlat=nlats)\n",
    "    \n",
    "    # compute minimum of BSF on this index \n",
    "    index = crossection_ds.min(('nlon','nlat')).values\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "487c6d4f-c75a-4227-9dd5-a05e5ed76bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file  0 / 82  executed\n",
      "execution finished\n"
     ]
    }
   ],
   "source": [
    "### COMPUTATION ###\n",
    "\n",
    "# compute location cross section\n",
    "ds = xr.open_dataset(files[0]).isel(time=0)\n",
    "east_nlats, east_nlons = cross_section(ds, point_OSNAP_center, point_OSNAP_east, 60)\n",
    "west_nlats, west_nlons = cross_section(ds, point_OSNAP_west, point_OSNAP_center, 40)\n",
    "\n",
    "# loop through list of files\n",
    "for i in range(1):#len(files)):\n",
    "    # read in files and apply mask\n",
    "    ds = xr.open_dataset(files[i]).where(mask3d == 1)\n",
    "\n",
    "    ds_min = ds.BSF.min(('nlon','nlat')) # find minimum of BSF in region per time step\n",
    "    time_series_min[:,i] = ds_min.values # store numpy series\n",
    "    \n",
    "    # compute OSNAP index for east and west section\n",
    "    time_series_east[:,i] = create_BSF_index(ds.BSF, east_nlats, east_nlons)\n",
    "    time_series_west[:,i] = create_BSF_index(ds.BSF, west_nlats, west_nlons)\n",
    "    \n",
    "    print('file ', str(i), '/', str(len(files)), ' executed')\n",
    "    \n",
    "print('execution finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78d1a63-a983-4d51-9e23-5cce9075e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OUTPUT ###\n",
    "\n",
    "# Save time series to a single file\n",
    "np.save(\"bsf_min_time_series.npy\", time_series_min)\n",
    "np.save(\"OSNAPeast_time_series.npy\", time_series_east)\n",
    "np.save(\"OSNAPwest_time_series.npy\", time_series_west)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
